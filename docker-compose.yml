services:

  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@example.com
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "8082:80"
    restart: always
    networks:
      - my_shared_network
 
  spark-master:
    #image: bitnami/spark:3.5.1
    build: 
      context: ./spark
      dockerfile: Dockerfile 
    container_name: spark-master
    ports:
      - 8084:8080  
      - 7077:7077
    restart: always
    volumes:
      - ./orchestrator/dags/spark_jobs:/opt/bitnami/spark/spark_jobs
      - ./spark/conf/log4j.properties:/opt/bitnami/spark/conf/log4j.properties # config log spark
      - ./spark/conf/spark-env.sh:/opt/bitnami/spark/conf/spark-env.sh # config spark
      - ./spark/conf/spark-env.cmd:/opt/bitnami/spark/conf/spark-env.cmd # config spark
      - ./spark/events:/opt/bitnami/spark/events
      - ./spark/conf/core-site.xml:/opt/bitnami/spark/conf/core-site.xml # config pour la connection spark hadoop
      - ./spark/conf/hdfs-site.xml:/opt/bitnami/spark/conf/hdfs-site.xml # config pour la connection spark hadoop
      - ./spark/hadoop/bin:/opt/bitnami/spark/hadoop/bin
      - ./spark/postgres-jar/postgresql-42.7.3.jar:/opt/bitnami/spark/jars/postgresql-42.7.3.jar # config pour pour la connection spark postgresql
    networks:
      - my_shared_network
  spark-worker:
    #image: bitnami/spark:3.5.1
    build: 
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-worker
    restart: always
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_MODE=worker
    ports:
      - 8085:8081
    networks:
      - my_shared_network

  spark-worker-1:
    #image: bitnami/spark:3.5.1
    build: 
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-worker-1
    restart: always
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_MODE=worker
    ports:
      - 8086:8086
    networks:
      - my_shared_network
#hadoop
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    hostname: namenode
    container_name: namenode
    restart: always
    ports:
      - 9000:9000 
      - 9870:9870
    env_file:
      - ./hadoop.env
    volumes:
      - ./hadoop/data/namenode:/hadoop/dfs/data
    environment:
      - CLUSTER_NAME=test
    networks:
      - my_shared_network

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    ports:
      - 9864:9864
      - 9866:9866
    depends_on:
      - namenode
    env_file:
      - ./hadoop.env
    volumes:
      -  ./hadoop/data/datanode:/hadoop/dfs/data
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    networks:
      - my_shared_network   

#jupyter
  jupyter_notebook:
      image: jupyter/base-notebook 
      container_name: jupyter_notebook
      ports:
        - "8888:8888"
      restart: always
      volumes:
        - ./notebooks:/home/jovyan/work
        - ./spark/conf/core-site.xml:/usr/local/spark/conf/core-site.xml
        - ./spark/conf/hdfs-site.xml:/usr/local/spark/conf/hdfs-site.xml
        - ./spark/postgres-jar/postgresql-42.7.3.jar:/usr/local/spark/jars/postgresql-42.7.3.jar
        - ./spark/java:/opt/bitnami/java
      environment:
        - JUPYTER_ENABLE_LAB=yes
        - SPARK_MASTER=spark://spark-master:7077
        - ENV JUPYTERHUB_SERVICE_URL=http://localhost:8000
        - JAVA_HOME=/opt/bitnami/java
      command: start-notebook.sh --NotebookApp.token='' --NotebookApp.password=''
      depends_on:
        - spark-master
        - namenode
      networks:
        - my_shared_network

  #streamlit
  streamlit:
    build: 
      context: ./streamlit
      dockerfile: Dockerfile
    container_name: streamlit
    ports:
      - "8501:8501"
    volumes:
      - ./streamlit/app.py:/app/app.py
    restart: always
    environment:
      - PYTHONUNBUFFERED=1
    networks:
      - my_shared_network


networks:
  my_shared_network:
    external: true